---
title: "PVS analysis"
author: "Nkosi Sampson"
date: "2024-08-19"
output:
  html_document: default
  pdf_document: default
---


```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r, results  = 'hide', message=FALSE, warning=FALSE}
library(readxl)
library(ggplot2)
library(reshape2)
library(ggpubr)
library(dplyr)
library(corrplot)
library(Hmisc)
```


```{r, results  = 'hide', message=FALSE, warning=FALSE}
t_lm <- readxl::read_excel("LASI_DAD_neuroimaging_withIDs.xlsx")
wmpv<-  readr::read_csv("wmPVSnew.csv")
bgpv<-  readr::read_csv("bgPVSnew.csv")
```

```{r, results  = 'hide', message=FALSE, warning=FALSE}
pvsNew <- left_join(wmpv, bgpv, "Subject")
pvsNew$wm_vol = pvsNew$wm_volume
pvsNew$bg_vol = pvsNew$bg_volume
pvsNew$pvs_wm_vol = pvsNew$wm_pvs_volume
pvsNew$pvs_bg_vol = pvsNew$bg_pvs_volume
```


##### Data Cleaning ######################################################################################################################################################################################################################################################################################################

```{r}

# Calculate the mean for each variable for each BaseID
vars_to_average <- c('pvs_wm_vol', 'wm_vol', 'pvs_bg_vol', 'bg_vol')

# Extract the base ID (removing the '_1' or '_2' suffix)
pvsNew <- pvsNew %>%
  mutate(BaseID = sub("_[12]$", "", Subject)) %>% 
  group_by(BaseID) %>%
  summarise(across(all_of(vars_to_average), mean, na.rm = TRUE))
```

#There are 126 observations in the t_lm dataste and 118 in the WMPav datset. Which ones are in the one and not the other?
```{r}
# Identify which BaseIDs in t_lm are not in WMPVav
missing_ids1 <- setdiff(t_lm$SubjectID, pvsNew$BaseID)
print(missing_ids1)
```

#Looks like there are also some observations in the WMPVav dataset that aren't in the t_lm dataset. These are:
```{r}
# Identify which BaseIDs in t_lm are not in WMPVav
missing_ids2 <- setdiff( pvsNew$BaseID, t_lm$SubjectID)
print(missing_ids2)
```


```{r}
# Filter out the scans from t_lm that are not in WMPVav
filtered_t_lm <- t_lm %>%
  filter(!SubjectID %in% missing_ids1)

# Display the resulting filtered dataset
print(filtered_t_lm)


# Filter out the scans from t_lm that are not in WMPVav
filtered_pvsNew <- pvsNew %>%
  filter(!BaseID %in% missing_ids2)

# Display the resulting filtered dataset
print(filtered_pvsNew)
```

```{r}
df <- left_join(filtered_t_lm, filtered_pvsNew, by = join_by(SubjectID == BaseID))

print(df)
```

```{r}
cdr <- readxl::read_excel("H_DAD_mri_cdr_v2.xlsx")
```


```{r}
df <- left_join(df, cdr, by = join_by(prim_key == prim_key))
print(df)
```

```{r}
df$cdrnew = ifelse(df$cdr == "0", 0, 1)
df$LOGpvs_bg_vol = log(df$pvs_bg_vol)
df$LOGpvs_wm_vol = log(df$pvs_wm_vol)
df$LOGAirPollution_2016 = log(df$AirPollution_2016)
df$vol_cortex = log(df$vol_cortex)
```

```{r}
#df <- filter(df, cdrnew == "0")
```

```{r}
# List of all fields including the response variable
all_fields <- c('Age', 'Sex', 'EducationYears', 'wm_vol', 'eTIV', 'LOGpvs_bg_vol', 'bg_vol', 'AirPollution_2016', 'vol_cortex', 'vol_hippocampus', 'HearingTest_l', 'HearingTest_r', 'EducationYears')

dont_standardize <- c('Urban', 'Literate', 'PreparesHotMeal', 'UsesPublicTransport', 'UsesUncleanCookingFuel', 'UsesUncleanHouseholdFuel')


# Custom standardization functionz
nanzscore <- function(x) {
  return((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
}

# Standardize the data
df <- df %>%
  mutate(across(all_fields[!all_fields %in% dont_standardize], nanzscore))
```




#Check Assumptions of linear regression


```{r, results  = 'hide', message=FALSE, warning=FALSE}
library(ggplot2)

# List of all fields excluding the response variable
all_fields <- c('EducationYears', 'BMI', 'DiastolicBP', 
                'Nap_HoursPerDay', 
                'Chores_HoursPerDay', 'EducationYears', 
                'HearingTest_l', 'HearingTest_r', 'Urban', 'SystolicBP','Chores_HoursPerDay', 'StoreFrequency', 'WorkFrequency', 'Depressed_TimesPerWeek', 'Nap_HoursPerDay', 'ExerciseFrequency', 'AirPollution_2016',
                'Lonely_TimesPerWeek', 'TV_HoursPerDay', 'Reading_HoursPerDay', 
                'WalkingFrequency', 'Computer_HoursPerDay', 'smoking_per_day', 'AlcoholFrequency')

# Function to create partial residual plots for a given variable
create_partial_residual_plot <- function(variable, full_model_formula, df) {
  
  # Skip the "Sex" variable
  if (variable == "Sex" ) {
    print("Skipping variable 'Sex'")
    return(NULL)
  }
  
  # Define the reduced model formula by removing the current variable
  reduced_model_formula <- as.formula(paste("LOGpvs_bg_vol ~", paste(setdiff(all.vars(full_model_formula), c("LOGpvs_bg_vol", variable)), collapse = " + ")))
  
  # Use complete cases to ensure no missing values
  complete_cases <- complete.cases(df[all.vars(full_model_formula)])
  df_complete <- df[complete_cases, ]
  
  # Fit the models and handle errors
  full_model <- tryCatch(lm(full_model_formula, data = df_complete), error = function(e) NULL)
  reduced_model <- tryCatch(lm(reduced_model_formula, data = df_complete), error = function(e) NULL)
  
  # If the models cannot be fitted, skip this variable
  if (is.null(full_model) || is.null(reduced_model)) {
    print(paste("Skipping variable due to singularity or other issues:", variable))
    return(NULL)
  }
  
  # Calculate residuals from the reduced model
  residuals_reduced <- resid(reduced_model)
  
  # Extract the coefficient for the predictor of interest from the full model
  beta_X <- tryCatch(coef(full_model)[variable], error = function(e) NULL)
  
  # Check if the coefficient could be extracted
  if (is.null(beta_X)) {
    print(paste("Skipping variable due to coefficient extraction issue:", variable))
    return(NULL)
  }
  
  # Extract the values of the predictor of interest
  X_values <- df_complete[[variable]]
  
  # Calculate partial residuals
  partial_residuals <- residuals_reduced + beta_X * X_values
  
  # Create the plot using ggplot2
  p <- ggplot(data.frame(X_values, partial_residuals), aes(x = X_values, y = partial_residuals)) +
    geom_point() +
    geom_smooth(method = "lm", col = "red") +
    labs(x = variable, y = "Partial Residuals", title = paste("Partial Residual Plot for", variable))
  
  return(p)
}

# Store all plots in a list
plot_list <- list()

# Outer loop through each variable in the list to build the full model
for (full_model_variable in all_fields) {
  full_model_formula <- as.formula(paste("LOGpvs_bg_vol ~ Age + Sex + eTIV + bg_vol +", full_model_variable))
  
  # Inner loop to create partial residual plots for each variable in the full model
  for (variable in c('Age', 'Sex',  'eTIV', 'bg_vol', full_model_variable)) {
    plot <- create_partial_residual_plot(variable, full_model_formula, df)
    if (!is.null(plot)) {
      plot_list <- c(plot_list, list(plot))
    }
  }
}

# Save all plots to a multi-page PDF file
pdf("partial_residual_plots.pdf", width = 8.5, height = 11) # Standard page size, adjust if needed
for (p in plot_list) {
  print(p)
}
dev.off()

```






```{r}
library(car)

# List of all fields
all_fields <- c( 'EducationYears', 'Age', 'Sex', 'eTIV', 'bg_vol', 'logWMHvol', 'SystolicBP', 'BMI', 'Chores_HoursPerDay', 'Urban', 'DiastolicBP', 'StoreFrequency', 
                'UsesPublicTransport', 'WorkFrequency', 'Depressed_TimesPerWeek', 'Literate', 
                'PreparesHotMeal', 'AirPollution_2016', 'Nap_HoursPerDay', 'ExerciseFrequency', 
                'Lonely_TimesPerWeek', 'UsesUncleanHouseholdFuel',  'TV_HoursPerDay', 'Reading_HoursPerDay', 
                'WalkingFrequency', 'EducationYears', 'UsesUncleanCookingFuel', 
                'smoking_per_day', 'HearingTest', 'AlcoholFrequency', 'Urban', 'Literate', 'PreparesHotMeal', 'UsesPublicTransport', 'UsesUncleanCookingFuel', 'UsesUncleanHouseholdFuel')

# Loop through each variable in the list
for (variable in all_fields) {
  # Define the formula for the linear model
  formula <- as.formula(paste("LOGpvs_wm_vol ~ Age + Sex +  eTIV + wm_vol +", variable))
  
  # Fit the linear model
  lm_model <- lm(formula, data = df)
  
  #plot(resid(lm_model))
  
  # Perform the Kolmogorov-Smirnov test
  ks_test <- ks.test(resid(lm_model), "pnorm", mean = mean(resid(lm_model)), sd = sd(resid(lm_model)))
  
  # Calculate the VIF values
  vif_values <- vif(lm_model)
  
  # Check for p-value < 0.05
  if (ks_test$p.value < 0.05) {
    print(paste("Warning: p-value < 0.05 for variable:", variable))
  }
  
  # Check for VIF > 5
  if (any(vif_values > 5)) {
    print(paste("Warning: VIF > 5 for variable:", variable))
  }
}
```


### This process was repeated for the model with LOGpvs_bg_vol as the response variable








#Additional R squared analaysis
```{r}
# List of all fields including the response variable
all_fields <- c('EducationYears', 'AirPollution_2016','Age', 'Sex', 'wm_vol', 'SystolicBP', 'BMI', 'Chores_HoursPerDay', 'Urban', 'DiastolicBP', 'StoreFrequency', 
                'UsesPublicTransport', 'WorkFrequency', 'Depressed_TimesPerWeek', 'Literate', 
                'PreparesHotMeal', 'Nap_HoursPerDay', 'ExerciseFrequency', 
                'Lonely_TimesPerWeek', 'UsesUncleanHouseholdFuel', 'TV_HoursPerDay', 'Reading_HoursPerDay', 
                'WalkingFrequency', 'EducationYears', 'Computer_HoursPerDay', 'UsesUncleanCookingFuel', 
                'smoking_per_day', 'HearingTest', 'AlcoholFrequency')

dont_standardize <- c('Urban', 'Literate', 'PreparesHotMeal', 'UsesPublicTransport', 'UsesUncleanCookingFuel', 'UsesUncleanHouseholdFuel')


# Custom standardization function
nanzscore <- function(x) {
  return((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
}

# Standardize the data
df_std <- df %>%
  mutate(across(all_fields[!all_fields %in% dont_standardize], nanzscore))


# Baseline model
mdl_baseline <- lm(LOGpvs_wm_vol ~ Age + Sex + eTIV + wm_vol + logWMHvol, data = df_std)
summary(mdl_baseline)

# Calculate baseline R-squared
explain_variance_baseline <- summary(mdl_baseline)$r.squared
```



```{r}
all_fields_predictors <- all_fields

# Initialize output dataframe
pvsWM_out_all <- data.frame(field_name = all_fields_predictors,
                            add_Rsquared = numeric(length(all_fields_predictors)),
                            t = numeric(length(all_fields_predictors)),
                            p = numeric(length(all_fields_predictors)))

# Analyze each field
for (i in seq_along(all_fields_predictors)) {
  formula_str <- paste('LOGpvs_wm_vol ~ Age + Sex + eTIV + wm_vol + logWMHvol +', all_fields_predictors[i])
  mdl_i <- lm(as.formula(formula_str), data = df_std)
  summary_i <- summary(mdl_i)
  
  # Extract coefficient index correctly (depends on how many predictors you have)
  coeff_index <- which(names(coef(mdl_i)) == all_fields_predictors[i])
  
  pvsWM_out_all$t[i] <- coef(summary_i)[coeff_index, "t value"]
  pvsWM_out_all$p[i] <- coef(summary_i)[coeff_index, "Pr(>|t|)"]
  pvsWM_out_all$add_Rsquared[i] <- round((summary_i$r.squared - explain_variance_baseline) * 100, 1)
}

# Adjust p-values using Benjamini-Hochberg method
pvsWM_out_all$p_wo_rural_FDR <- p.adjust(pvsWM_out_all$p, method = "BH")

# Sort by additional R-squared
pvsWM_out_all <- pvsWM_out_all[order(-pvsWM_out_all$add_Rsquared), ]

# Display the additional R-squared table
print(pvsWM_out_all)
 summary(lm(LOGpvs_wm_vol ~ Age + Sex +  eTIV + wm_vol + logWMHvol, df_std))
```

```{r}
# List of all fields including the response variable
all_fields <- c('EducationYears', 'AirPollution_2016','Age', 'Sex', 'wm_vol', 'SystolicBP', 'BMI', 'Chores_HoursPerDay', 'Urban', 'DiastolicBP', 'StoreFrequency', 
                'UsesPublicTransport', 'WorkFrequency', 'Depressed_TimesPerWeek', 'Literate', 
                'PreparesHotMeal', 'Nap_HoursPerDay', 'ExerciseFrequency', 
                'Lonely_TimesPerWeek', 'UsesUncleanHouseholdFuel', 'TV_HoursPerDay', 'Reading_HoursPerDay', 
                'WalkingFrequency', 'EducationYears', 'Computer_HoursPerDay', 'UsesUncleanCookingFuel', 
                'smoking_per_day', 'HearingTest', 'AlcoholFrequency')

dont_standardize <- c('Urban', 'Literate', 'PreparesHotMeal', 'UsesPublicTransport', 'UsesUncleanCookingFuel', 'UsesUncleanHouseholdFuel')


# Custom standardization function
nanzscore <- function(x) {
  return((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
}

# Standardize the data
df_std <- df %>%
  mutate(across(all_fields[!all_fields %in% dont_standardize], nanzscore))


# Baseline model
mdl_baseline <- lm(LOGpvs_bg_vol ~ Age + Sex + eTIV + bg_vol, data = df_std)
summary(mdl_baseline)

# Calculate baseline R-squared
explain_variance_baseline <- summary(mdl_baseline)$r.squared
```



```{r}
all_fields_predictors <- all_fields

# Initialize output dataframe
pvsWM_out_all <- data.frame(field_name = all_fields_predictors,
                            add_Rsquared = numeric(length(all_fields_predictors)),
                            t = numeric(length(all_fields_predictors)),
                            p = numeric(length(all_fields_predictors)))

# Analyze each field
for (i in seq_along(all_fields_predictors)) {
  formula_str <- paste('LOGpvs_bg_vol ~ Age + Sex + eTIV + bg_vol +', all_fields_predictors[i])
  mdl_i <- lm(as.formula(formula_str), data = df_std)
  summary_i <- summary(mdl_i)
  
  # Extract coefficient index correctly (depends on how many predictors you have)
  coeff_index <- which(names(coef(mdl_i)) == all_fields_predictors[i])
  
  pvsWM_out_all$t[i] <- coef(summary_i)[coeff_index, "t value"]
  pvsWM_out_all$p[i] <- coef(summary_i)[coeff_index, "Pr(>|t|)"]
  pvsWM_out_all$add_Rsquared[i] <- round((summary_i$r.squared - explain_variance_baseline) * 100, 1)
}

# Adjust p-values using Benjamini-Hochberg method
pvsWM_out_all$p_wo_rural_FDR <- p.adjust(pvsWM_out_all$p, method = "BH")

# Sort by additional R-squared
pvsWM_out_all <- pvsWM_out_all[order(-pvsWM_out_all$add_Rsquared), ]

# Display the additional R-squared table
print(pvsWM_out_all)
 summary(lm(LOGpvs_bg_vol ~ Age + Sex +  eTIV + bg_vol, df_std))
```


# Mediation Analysis

### Assumptions of linear regression were checked with the same methods as before, for each path of the mediation analysis. 

```{r}
# List of all fields including the response variable
all_fields <- c('Age', 'Sex', 'EducationYears', 'wm_vol', 'eTIV', 'LOGpvs_wm_vol', 'bg_vol', 'AirPollution_2016', 'vol_cortex', 'vol_hippocampus', 'HearingTest_l', 'HearingTest_r', 'EducationYears')

dont_standardize <- c('Urban', 'Literate', 'PreparesHotMeal', 'UsesPublicTransport', 'UsesUncleanCookingFuel', 'UsesUncleanHouseholdFuel')


# Custom standardization functionz
nanzscore <- function(x) {
  return((x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE))
}

# Standardize the data
df_std <- df %>%
  mutate(across(all_fields[!all_fields %in% dont_standardize], nanzscore))
```

```{r}
library(lavaan)

set.seed(01)

model1 <- "
# Path c' (direct effect)
LOGpvs_wm_vol ~ c*Urban + Age + Sex + EducationYears + eTIV  + wm_vol + logWMHvol

#Path a 
AirPollution_2016 ~ a*Urban + Age + Sex + EducationYears + eTIV + wm_vol + logWMHvol

#Path b 
LOGpvs_wm_vol ~ b*AirPollution_2016 + Age + Sex + EducationYears + eTIV + wm_vol + logWMHvol

#Indirect Effect (a*b)
ab := a*b
"

# Fit the model with bootstrap resampling
fitmod1 <- sem(model1, data = df_std)

# Summarize the results with BCA confidence intervals
summary(fitmod1, fit.measures = TRUE, rsquare = TRUE, ci = TRUE)

```

```{r}
# Fit separate models for paths a and b
fit_a <- lm(AirPollution_2016 ~ Urban + Age + Sex + EducationYears + eTIV + wm_vol + logWMHvol, data = df)
fit_b <- lm(LOGpvs_wm_vol ~ AirPollution_2016 + Age + Sex + EducationYears + eTIV + wm_vol + logWMHvol, data = df)

# Extract residuals
residuals_a <- residuals(fit_a)
residuals_b <- residuals(fit_b)

# Calculate correlation between residuals
correlation <- cor(residuals_a, residuals_b)

# Print the correlation
print(correlation)

```


```{r}
library(RMediation)

medci(mu.x = 18.76, mu.y = .03028, se.x = 2.765, se.y = 0.003539, rho =  -0.03475169, alpha = 0.05, type = "prodclin")
```

```{r}
# Assuming your dataframe is named df

# Fit the linear model
model <- lm(LOGpvs_wm_vol ~ Urban + Age + Sex + EducationYears + eTIV + wm_vol + logWMHvol, data = df)

# Calculate the confidence intervals for the coefficients
conf_intervals <- confint(model)

# Extract the confidence interval for Urban
conf_intervals["Urban", ]

```




```{r}
# Load necessary package
library(boot)

# Fit the models
model_c <- lm(LOGpvs_wm_vol ~ Urban + Age + Sex +  eTIV + wm_vol + logWMHvol, data = df)
model_a <- lm(AirPollution_2016 ~ Urban + Age + Sex + eTIV + wm_vol + logWMHvol, data = df)
model_b <- lm(LOGpvs_wm_vol ~ AirPollution_2016 + Urban + Age + Sex + eTIV + wm_vol + logWMHvol, data = df)

# Extract coefficients
a <- coef(model_a)["Urban"]
b <- coef(model_b)["AirPollution_2016"]
c <- coef(model_c)["Urban"]
c_prime <- coef(model_b)["Urban"]

# Compute effects
indirect_effect <- a * b
direct_effect <- c_prime
total_effect <-  abs(direct_effect) + abs(indirect_effect)
proportion_mediated <- (abs(total_effect) - abs(direct_effect)) / abs(total_effect)

# Define a function to compute the effects for bootstrapping
boot_function <- function(data, indices) {
  d <- data[indices, ]  # Resample with replacement
  model_a <- lm(AirPollution_2016 ~ Urban + Age + Sex + eTIV + wm_vol + logWMHvol, data = d)
  model_b <- lm(LOGpvs_wm_vol ~ AirPollution_2016 + Urban + Age + Sex + eTIV + wm_vol + logWMHvol, data = d)
  model_c <- lm(LOGpvs_wm_vol ~ Urban + Age + Sex + eTIV + wm_vol + logWMHvol, data = d)
  
  a <- coef(model_a)["Urban"]
  b <- coef(model_b)["AirPollution_2016"]
  c <- coef(model_c)["Urban"]
  c_prime <- coef(model_b)["Urban"]
  
  indirect_effect <- a * b
  direct_effect <- c_prime
  total_effect <-  abs(direct_effect) + abs(indirect_effect)
  proportion_mediated <- (abs(total_effect) - abs(direct_effect)) / abs(total_effect)
  
  return(c(indirect_effect, direct_effect, total_effect, proportion_mediated))
}

# Perform bootstrapping
set.seed(123)
boot_results <- boot(data = df, statistic = boot_function, R = 10000)

# Extract bootstrap estimates
bootstrap_estimates <- boot_results$t

# Calculate 95% confidence intervals
a_ci <- quantile(bootstrap_estimates[,1], c(0.025, 0.975))
b_ci <- quantile(bootstrap_estimates[,2], c(0.025, 0.975))
indirect_effect_ci <- quantile(bootstrap_estimates[,1], c(0.025, 0.975))
direct_effect_ci <- quantile(bootstrap_estimates[,2], c(0.025, 0.975))
total_effect_ci <- quantile(bootstrap_estimates[,3], c(0.025, 0.975))
proportion_mediated_ci <- quantile(bootstrap_estimates[,4], c(0.025, 0.975))

# Display results
print("Path a (Urban -> AirPollution_2016):")
print(a)
print("95% CI for Path a:")
print(a_ci)

print("Path b (AirPollution_2016 -> LOGpvs_bg_vol):")
print(b)
print("95% CI for Path b:")
print(b_ci)

# Display results
print("Indirect Effect (a*b):")
print(indirect_effect)
print("95% CI for Indirect Effect:")
print(indirect_effect_ci)

print("Direct Effect (c'):")
print(direct_effect)
print("95% CI for Direct Effect:")
print(direct_effect_ci)

print("Total Effect (c):")
print(total_effect)
print("95% CI for Total Effect:")
print(total_effect_ci)

print("Proportion Mediated (c-c`/c):")
print(proportion_mediated)
print("95% CI for Proportion Mediated:")
print(proportion_mediated_ci)



# Extract paths a and b from the bootstrap results
a_path <- bootstrap_estimates[, 1]
b_path <- bootstrap_estimates[, 2]

# Plot histogram for path a
hist(a_path, main="Histogram of Path a", xlab="Path a", col="blue", breaks=30)

# Plot histogram for path b
hist(b_path, main="Histogram of Path b", xlab="Path b", col="green", breaks=30)


```


```{r}
# Load necessary package
library(boot)

# Fit the models
model_c <- lm(LOGpvs_bg_vol ~ Urban + Age + Sex +  eTIV + bg_vol, data = df)
model_a <- lm(AirPollution_2016 ~ Urban + Age + Sex + eTIV + bg_vol, data = df)
model_b <- lm(LOGpvs_bg_vol ~ AirPollution_2016 + Urban + Age + Sex + eTIV + bg_vol, data = df)

# Extract coefficients
a <- coef(model_a)["Urban"]
b <- coef(model_b)["AirPollution_2016"]
c <- coef(model_c)["Urban"]
c_prime <- coef(model_b)["Urban"]

# Compute effects
indirect_effect <- a * b
direct_effect <- c_prime
total_effect <- abs(direct_effect) + abs(indirect_effect)
proportion_mediated <- (abs(total_effect) - abs(direct_effect)) / abs(total_effect)

# Define a function to compute the effects for bootstrapping
boot_function <- function(data, indices) {
  d <- data[indices, ]  # Resample with replacement
  model_a <- lm(AirPollution_2016 ~ Urban + Age + Sex + EducationYears + eTIV + bg_vol, data = d)
  model_b <- lm(LOGpvs_bg_vol ~ AirPollution_2016 + Urban + Age + Sex + EducationYears + eTIV + bg_vol, data = d)
  model_c <- lm(LOGpvs_bg_vol ~ Urban + Age + Sex + EducationYears + eTIV + bg_vol, data = d)
  
  a <- coef(model_a)["Urban"]
  b <- coef(model_b)["AirPollution_2016"]
  c <- coef(model_c)["Urban"]
  c_prime <- coef(model_b)["Urban"]
  
  indirect_effect <- a * b
  direct_effect <- c_prime
  total_effect <-  abs(direct_effect) + abs(indirect_effect)
  proportion_mediated <- (abs(total_effect) - abs(direct_effect)) / abs(total_effect)
  
  return(c(indirect_effect, direct_effect, total_effect, proportion_mediated))
}

# Perform bootstrapping
set.seed(123)
boot_results <- boot(data = df, statistic = boot_function, R = 10000)

# Extract bootstrap estimates
bootstrap_estimates <- boot_results$t

# Calculate 95% confidence intervals
a_ci <- quantile(bootstrap_estimates[,1], c(0.025, 0.975))
b_ci <- quantile(bootstrap_estimates[,2], c(0.025, 0.975))
indirect_effect_ci <- quantile(bootstrap_estimates[,1], c(0.025, 0.975))
direct_effect_ci <- quantile(bootstrap_estimates[,2], c(0.025, 0.975))
total_effect_ci <- quantile(bootstrap_estimates[,3], c(0.025, 0.975))
proportion_mediated_ci <- quantile(bootstrap_estimates[,4], c(0.025, 0.975))

# Display results
print("Path a (Urban -> AirPollution_2016):")
print(a)
print("95% CI for Path a:")
print(a_ci)

print("Path b (AirPollution_2016 -> LOGpvs_bg_vol):")
print(b)
print("95% CI for Path b:")
print(b_ci)

# Display results
print("Indirect Effect (c-c`):")
print(indirect_effect)
print("95% CI for Indirect Effect:")
print(indirect_effect_ci)

print("Direct Effect (c'):")
print(direct_effect)
print("95% CI for Direct Effect:")
print(direct_effect_ci)

print("Total Effect (c):")
print(total_effect)
print("95% CI for Total Effect:")
print(total_effect_ci)

print("Proportion Mediated (a*b/c):")
print(proportion_mediated)
print("95% CI for Proportion Mediated:")
print(proportion_mediated_ci)

# Extract paths a and b from the bootstrap results
a_path <- bootstrap_estimates[, 1]
b_path <- bootstrap_estimates[, 2]

# Plot histogram for path a
hist(a_path, main="Histogram of Path a", xlab="Path a", col="blue", breaks=30)

# Plot histogram for path b
hist(b_path, main="Histogram of Path b", xlab="Path b", col="green", breaks=30)

```

